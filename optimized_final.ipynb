{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samj786/NCVPRIPG-AutoEval/blob/main/optimized_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88_DTzzYnTMb"
      },
      "source": [
        "# Install Required Packages\n",
        "First, we need to install the required packages: `pytesseract`, `tesseract-ocr`, `opencv`, and `transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "collapsed": true,
        "id": "YP-il7w7nbkW",
        "outputId": "39437e46-1c67-49a4-8201-800482d6cedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,230 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install python-doctr\\n!pip install tf2onnx\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr\n",
        "\n",
        "'''\n",
        "!pip install python-doctr\n",
        "!pip install tf2onnx\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIY4ya7Knv5B"
      },
      "source": [
        "# Import Libraries\n",
        " Import the necessary libraries for image processing, OCR, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2Gl4V4YnyjT"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import pytesseract\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as display\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForObjectDetection, TableTransformerForObjectDetection\n",
        "from torchvision import transforms\n",
        "from matplotlib.patches import Rectangle, Patch\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "from tqdm.auto import tqdm\n",
        "import glob\n",
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "sw-ZeoBbBAr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom resize transformation\n",
        "class MaxResize(object):\n",
        "    def __init__(self, max_size=800):\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        width, height = image.size\n",
        "        current_max_size = max(width, height)\n",
        "        scale = self.max_size / current_max_size\n",
        "        resized_image = image.resize((int(round(scale * width)), int(round(scale * height))))\n",
        "        return resized_image\n",
        "\n",
        "# Postprocessing functions\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def outputs_to_objects(outputs, img_size, id2label):\n",
        "    m = outputs.logits.softmax(-1).max(-1)\n",
        "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
        "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
        "    pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n",
        "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
        "\n",
        "    objects = []\n",
        "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
        "        class_label = id2label[int(label)]\n",
        "        if not class_label == 'no object':\n",
        "            objects.append({'label': class_label, 'score': float(score), 'bbox': [float(elem) for elem in bbox]})\n",
        "    return objects\n",
        "\n",
        "# Function to get model outputs\n",
        "def get_model_outputs(image, model, transform):\n",
        "    pixel_values = transform(image).unsqueeze(0)\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# Visualization function for table region\n",
        "def fig2img(fig):\n",
        "    import io\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight', dpi=150)\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf)\n",
        "\n",
        "\n",
        "def visualize_detected_objects(img, objects, out_path=None):\n",
        "    # Optimize figure creation\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(img, interpolation=\"lanczos\")\n",
        "\n",
        "    for obj in objects:\n",
        "        bbox = obj['bbox']\n",
        "        label = obj['label']\n",
        "        facecolor = (1, 0, 0.45) if label == 'table' else (0.95, 0.6, 0.1)\n",
        "        edgecolor = facecolor\n",
        "        alpha = 0.3\n",
        "        linewidth = 2\n",
        "        hatch = '//////' if label == 'table' else 'xxxx'\n",
        "\n",
        "        # Add patches in a more efficient manner\n",
        "        rect_params = {'linewidth': linewidth, 'edgecolor': 'none', 'facecolor': facecolor, 'alpha': 0.1}\n",
        "        ax.add_patch(Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], **rect_params))\n",
        "\n",
        "        rect_params.update({'edgecolor': edgecolor, 'facecolor': 'none', 'alpha': alpha})\n",
        "        ax.add_patch(Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], **rect_params))\n",
        "\n",
        "        rect_params.update({'linewidth': 0, 'hatch': hatch, 'alpha': 0.2})\n",
        "        ax.add_patch(Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], **rect_params))\n",
        "\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.axis('off')\n",
        "\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=(1, 0, 0.45), edgecolor=(1, 0, 0.45), label='Table', hatch='//////', alpha=0.3),\n",
        "        Patch(facecolor=(0.95, 0.6, 0.1), edgecolor=(0.95, 0.6, 0.1), label='Table (rotated)', hatch='xxxx', alpha=0.3)\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, bbox_to_anchor=(0.5, -0.02), loc='upper center', borderaxespad=0, fontsize=10, ncol=2)\n",
        "\n",
        "    if out_path is not None:\n",
        "        fig.savefig(out_path, bbox_inches='tight', dpi=150)\n",
        "\n",
        "    return fig\n",
        "\n",
        "#Helper function for crop table\n",
        "def iob(bbox1, bbox2):\n",
        "    \"\"\"Calculates Intersection over Union (IoU) for two bounding boxes.\"\"\"\n",
        "    x1 = max(bbox1[0], bbox2[0])\n",
        "    y1 = max(bbox1[1], bbox2[1])\n",
        "    x2 = min(bbox1[2], bbox2[2])\n",
        "    y2 = min(bbox1[3], bbox2[3])\n",
        "\n",
        "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "    bbox1_area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n",
        "    bbox2_area = (bbox2[2] - bbox2[0] + 1) * (bbox2[3] - bbox2[1] + 1)\n",
        "    union_area = bbox1_area + bbox2_area - intersection_area\n",
        "\n",
        "    return intersection_area / union_area\n",
        "\n",
        "#Crop table function\n",
        "def objects_to_crops(img, tokens, objects, class_thresholds, padding):\n",
        "    table_crops = []\n",
        "    for obj in objects:\n",
        "        if obj['score'] < class_thresholds[obj['label']]:\n",
        "            continue\n",
        "\n",
        "        cropped_table = {}\n",
        "\n",
        "        bbox = obj['bbox']\n",
        "        bbox = [bbox[0]-padding, bbox[1]-10, bbox[2]+(1.2*padding), bbox[3]+padding]\n",
        "\n",
        "        bbox[1] = max(0, bbox[1])\n",
        "\n",
        "        cropped_img = img.crop(bbox)\n",
        "\n",
        "        table_tokens = [token for token in tokens if iob(token['bbox'], bbox) >= 0.5]\n",
        "        for token in table_tokens:\n",
        "            token['bbox'] = [token['bbox'][0]-bbox[0],\n",
        "                             token['bbox'][1]-bbox[1],\n",
        "                             token['bbox'][2]-bbox[0],\n",
        "                             token['bbox'][3]-bbox[1]]\n",
        "\n",
        "        if obj['label'] == 'table rotated':\n",
        "            cropped_img = cropped_img.rotate(270, expand=True)\n",
        "            for token in table_tokens:\n",
        "                bbox = token['bbox']\n",
        "                bbox = [cropped_img.size[0]-bbox[3]-1,\n",
        "                        bbox[0],\n",
        "                        cropped_img.size[0]-bbox[1]-1,\n",
        "                        bbox[2]]\n",
        "                token['bbox'] = bbox\n",
        "\n",
        "        cropped_table['image'] = cropped_img\n",
        "        cropped_table['tokens'] = table_tokens\n",
        "\n",
        "        table_crops.append(cropped_table)\n",
        "\n",
        "    return table_crops\n",
        "\n",
        "#Visualization of the table structure being detected\n",
        "def plot_results(cells, class_to_visualize):\n",
        "    if class_to_visualize not in structure_model.config.id2label.values():\n",
        "      raise ValueError(\"Class should be one of the available classes\")\n",
        "\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(cropped_table)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for cell in cells:\n",
        "        score = cell[\"score\"]\n",
        "        bbox = cell[\"bbox\"]\n",
        "        label = cell[\"label\"]\n",
        "\n",
        "        if label == class_to_visualize:\n",
        "          xmin, ymin, xmax, ymax = tuple(bbox)\n",
        "\n",
        "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=\"red\", linewidth=3))\n",
        "          text = f'{cell[\"label\"]}: {score:0.2f}'\n",
        "          ax.text(xmin, ymin, text, fontsize=15,\n",
        "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "          plt.axis('off')\n",
        "\n",
        "# Function to get the last column bbox\n",
        "def get_last_column_bbox(cells):\n",
        "    columns = [entry for entry in cells if entry['label'] == 'table column']\n",
        "    columns.sort(key=lambda x: x['bbox'][0])\n",
        "    return columns[-1]['bbox']\n",
        "\n",
        "# Function to get cell coordinates by row within the last column\n",
        "def get_row_coordinates_within_column(column_image, original_bbox, rows):\n",
        "    rows.sort(key=lambda x: x['bbox'][1])\n",
        "    for row in rows:\n",
        "        row['bbox'] = [original_bbox[0], row['bbox'][1], original_bbox[2], row['bbox'][3]]\n",
        "    return rows\n",
        "\n",
        "# Function to check if the image is blank\n",
        "def is_blank_image(image, threshold=0.986):\n",
        "    \"\"\"\n",
        "    Check if an image is blank by analyzing the percentage of white pixels.\n",
        "    \"\"\"\n",
        "    #black threshold\n",
        "    black_threshold = 0.2\n",
        "    # Convert image to numpy array\n",
        "    image_array = np.array(image)\n",
        "    # Calculate the percentage of white pixels\n",
        "    white_pixels = np.sum(image_array == 255)\n",
        "    total_pixels = image_array.size\n",
        "    white_pixel_ratio = white_pixels / total_pixels\n",
        "    black_pixel_ratio = 1 - white_pixel_ratio\n",
        "    #print(f\"White pixel ratio: {white_pixel_ratio:.4f}, Black pixel ratio: {black_pixel_ratio:.4f}\")\n",
        "    if black_pixel_ratio > black_threshold:\n",
        "        return True\n",
        "    return white_pixel_ratio > threshold\n",
        "\n",
        "\n",
        "# Perform OCR using the TrOCR model.\n",
        "def ocr(image, processor, model):\n",
        "    image = image.convert('RGB')\n",
        "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    generated_ids = model.generate(pixel_values)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_text\n",
        "\n",
        "# Check if two strings are similar based on a given threshold.\n",
        "def is_similar(a, b, threshold=0.4):\n",
        "    return SequenceMatcher(None, a, b).ratio() > threshold\n",
        "\n",
        "# Normalize text by converting to lowercase and removing non-alphanumeric characters.\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z]', '', text)\n",
        "    return text\n",
        "\n",
        "def post_process_ocr_results(ocr_results):\n",
        "    \"\"\"\n",
        "    Post-process OCR results to clean up and correctly identify true and false values.\n",
        "    \"\"\"\n",
        "    cleaned_results = []\n",
        "    for result in ocr_results:\n",
        "\n",
        "\n",
        "        # Remove special characters and spaces\n",
        "        result_no_special = re.sub(r'[^a-zA-Z0-9]', '', result)\n",
        "\n",
        "        # Check if the result is just numbers\n",
        "        if result_no_special.isdigit():\n",
        "            cleaned_results.append(\"Blank Image\")\n",
        "            continue\n",
        "\n",
        "        # Normalize the result\n",
        "        normalized_result = normalize_text(result)\n",
        "\n",
        "        # Check if the result is empty after normalization\n",
        "        if normalized_result == '':\n",
        "            cleaned_results.append(\"Blank Image\")\n",
        "            continue\n",
        "\n",
        "        # Apply the new rules if similarity checks fail\n",
        "        if normalized_result.startswith('t'):\n",
        "            if len(normalized_result) == 1 or (len(normalized_result) > 1 and normalized_result[1] == 'r'):\n",
        "                cleaned_results.append(\"true\")\n",
        "                continue\n",
        "\n",
        "\n",
        "        if normalized_result.startswith('f'):\n",
        "            if len(normalized_result) == 1 or (len(normalized_result) > 1 and normalized_result[1] == 'a'):\n",
        "                cleaned_results.append(\"false\")\n",
        "                continue\n",
        "\n",
        "\n",
        "        # Check for similarity first\n",
        "        if is_similar(normalized_result, \"true\"):\n",
        "            cleaned_results.append(\"true\")\n",
        "            continue\n",
        "        elif is_similar(normalized_result, \"false\"):\n",
        "            cleaned_results.append(\"false\")\n",
        "            continue\n",
        "\n",
        "        if normalized_result == 'blankimage':\n",
        "            cleaned_results.append(\"Blank Image\")\n",
        "            continue\n",
        "\n",
        "        if 't' in normalized_result and not 'f' in normalized_result:\n",
        "            cleaned_results.append(\"true\")\n",
        "        elif 'f' in normalized_result and not 't' in normalized_result:\n",
        "            cleaned_results.append(\"false\")\n",
        "        elif 'r' in normalized_result and not 'l' in normalized_result:\n",
        "            cleaned_results.append(\"true\")\n",
        "        elif 'l' in normalized_result and not 'r' in normalized_result:\n",
        "            cleaned_results.append(\"false\")\n",
        "        elif 'a' in normalized_result and not 'r' in normalized_result:\n",
        "            cleaned_results.append(\"false\")\n",
        "        elif 'r' in normalized_result and not 'a' in normalized_result:\n",
        "            cleaned_results.append(\"true\")\n",
        "        elif 's' in normalized_result:\n",
        "            cleaned_results.append(\"false\")\n",
        "        else:\n",
        "            cleaned_results.append(\"Uncertain\")  # Handle uncertain cases\n",
        "\n",
        "    return cleaned_results\n"
      ],
      "metadata": {
        "id": "h5NOBJG3BLAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open the original image, enhance its contrast, and use OCR to detect and correct the rotation angle.\n",
        "#Then grayscale and remove noise from the original image.\n",
        "#Then perform table region detection and crop the table\n",
        "#Perform structure recognition and get co-ordinates of the last column and its rows\n",
        "#for every cell in the row, crop it and binarize it and then perform trOCR on it.\n",
        "#Finally clean the ocr results\n",
        "def process_image(image_path, model, structure_model, processor_trOCR, model_trOCR):\n",
        "    original_im = cv.imread(image_path)\n",
        "    gray_im = cv.cvtColor(original_im, cv.COLOR_BGR2GRAY)\n",
        "    _, binary_im = cv.threshold(gray_im, 128, 255, cv.THRESH_BINARY)\n",
        "    osd = pytesseract.image_to_osd(binary_im, output_type='dict')\n",
        "    rotate = int(osd['rotate'])\n",
        "\n",
        "    if rotate != 0:\n",
        "        (h, w) = original_im.shape[:2]\n",
        "        center = (w // 2, h // 2)\n",
        "        M = cv.getRotationMatrix2D(center, -rotate, 1.0)\n",
        "        abs_cos, abs_sin = abs(M[0, 0]), abs(M[0, 1])\n",
        "        bound_w, bound_h = int(h * abs_sin + w * abs_cos), int(h * abs_cos + w * abs_sin)\n",
        "        M[0, 2] += bound_w / 2 - center[0]\n",
        "        M[1, 2] += bound_h / 2 - center[1]\n",
        "        im_fixed = cv.warpAffine(original_im, M, (bound_w, bound_h))\n",
        "    else:\n",
        "        im_fixed = original_im\n",
        "\n",
        "    gray_img = cv.cvtColor(im_fixed, cv.COLOR_BGR2GRAY)\n",
        "    median = cv.medianBlur(gray_img, 5)\n",
        "\n",
        "    binary_pil = Image.fromarray(median).convert(\"RGB\")\n",
        "    outputs = get_model_outputs(binary_pil, model, detection_transform)\n",
        "    objects = outputs_to_objects(outputs, binary_pil.size, id2label)\n",
        "\n",
        "\n",
        "    # Visualization for table region being detected - uncomment below lines to visualize\n",
        "    '''\n",
        "    fig = visualize_detected_objects(binary_pil, objects)\n",
        "    visualized_image = fig2img(fig)\n",
        "    visualized_image.show()\n",
        "    '''\n",
        "    tokens = []\n",
        "    tables_crops = objects_to_crops(binary_pil, tokens, objects, detection_class_thresholds, padding=crop_padding)\n",
        "    cropped_table = tables_crops[0]['image'].convert(\"RGB\")\n",
        "\n",
        "    new_outputs = get_model_outputs(cropped_table, structure_model, structure_transform)\n",
        "    new_cells = outputs_to_objects(new_outputs, cropped_table.size, structure_id2label)\n",
        "\n",
        "    # Visualization for table structure being detected - uncomment below lines to visualize\n",
        "    '''\n",
        "    plot_results(final_cells, class_to_visualize=\"table row\")\n",
        "    plt.show()\n",
        "    '''\n",
        "\n",
        "    rows = [entry for entry in new_cells if entry['label'] == 'table row']\n",
        "\n",
        "    # Check the number of detected rows\n",
        "    if len(rows) <= 10:\n",
        "        table_bbox = objects[0]['bbox']\n",
        "        extended_bbox = [table_bbox[0], table_bbox[1], table_bbox[2], table_bbox[3] + 200]\n",
        "        #extended_cropped_table = binary_pil.crop([extended_bbox[0]- crop_padding, extended_bbox[1]-crop_padding, extended_bbox[2]+crop_padding, extended_bbox[3]+crop_padding])\n",
        "\n",
        "        # Use the same cropping function for the extended bounding box\n",
        "        extended_objects = [{'label': 'table', 'score': 1.0, 'bbox': extended_bbox}]\n",
        "        extended_tables_crops = objects_to_crops(binary_pil, tokens, extended_objects, detection_class_thresholds, padding=crop_padding)\n",
        "        extended_cropped_table = extended_tables_crops[0]['image'].convert(\"RGB\")\n",
        "\n",
        "        # Reapply the model on the extended cropped table\n",
        "        extended_outputs = get_model_outputs(extended_cropped_table, structure_model, structure_transform)\n",
        "        extended_cells = outputs_to_objects(extended_outputs, extended_cropped_table.size, structure_id2label)\n",
        "        rows = [entry for entry in extended_cells if entry['label'] == 'table row']\n",
        "        #print(f\"Number of detected rows: {len(rows)}\")\n",
        "        final_cropped_table = extended_cropped_table if len(rows) > 10 else cropped_table\n",
        "    else:\n",
        "        final_cropped_table = cropped_table\n",
        "\n",
        "    final_outputs = get_model_outputs(final_cropped_table, structure_model, structure_transform)\n",
        "    final_cells = outputs_to_objects(final_outputs, final_cropped_table.size, structure_id2label)\n",
        "\n",
        "    last_column_bbox = get_last_column_bbox(final_cells)\n",
        "    last_column_image = final_cropped_table.crop(last_column_bbox)\n",
        "    rows_within_last_column = get_row_coordinates_within_column(last_column_image, last_column_bbox, rows)\n",
        "\n",
        "    ocr_results = []\n",
        "    for idx, row in enumerate(rows_within_last_column):\n",
        "        cell_bbox = [last_column_bbox[0], row['bbox'][1], last_column_bbox[2], row['bbox'][3]]\n",
        "        cell_image = cropped_table.crop(cell_bbox)\n",
        "        grayscale_image = cell_image.convert('L')\n",
        "        image_array = np.array(grayscale_image)\n",
        "        _, binarized_image = cv.threshold(image_array, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
        "        binarized_image_pil = Image.fromarray(binarized_image)\n",
        "\n",
        "        if idx == 0:\n",
        "          continue;\n",
        "        if is_blank_image(binarized_image_pil):\n",
        "            ocr_results.append(\"Blank Image\")\n",
        "        else:\n",
        "            result = ocr(binarized_image_pil, processor_trOCR, model_trOCR)\n",
        "            ocr_results.append(result)\n",
        "\n",
        "    cleaned_results = post_process_ocr_results(ocr_results)\n",
        "    return cleaned_results"
      ],
      "metadata": {
        "id": "KVTEF_N0BWSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "XPv_3JEzGIml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "uEMETW6nxv79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    global device, detection_transform, structure_transform, id2label, structure_id2label, detection_class_thresholds, crop_padding\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    detection_transform = transforms.Compose([\n",
        "        MaxResize(800),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    structure_transform = transforms.Compose([\n",
        "        MaxResize(1000),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\", revision=\"no_timm\")\n",
        "    structure_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-structure-recognition-v1.1-all\")\n",
        "    processor_trOCR = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
        "    model_trOCR = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\n",
        "\n",
        "    model.to(device)\n",
        "    structure_model.to(device)\n",
        "    model_trOCR.to(device)\n",
        "\n",
        "    id2label = model.config.id2label\n",
        "    id2label[len(model.config.id2label)] = \"no object\"\n",
        "    structure_id2label = structure_model.config.id2label\n",
        "    structure_id2label[len(structure_id2label)] = \"no object\"\n",
        "\n",
        "    detection_class_thresholds = {\n",
        "        \"table\": 0.5,\n",
        "        \"table rotated\": 0.5,\n",
        "        \"no object\": 10\n",
        "    }\n",
        "    crop_padding = 200\n",
        "\n",
        "    correct_answers = []\n",
        "    with open('ModelAnswer (1).csv', 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header\n",
        "        for row in reader:\n",
        "            _, answer = row\n",
        "            correct_answers.append(answer)\n",
        "\n",
        "    # Define the folder containing the test images\n",
        "    test_image_folder = '/content/test_images'\n",
        "    results = []\n",
        "\n",
        "    for image_name in os.listdir(test_image_folder):\n",
        "        if image_name.endswith(('jpg', 'jpeg', 'png')):  # Process only image files\n",
        "            image_path = os.path.join(test_image_folder, image_name)\n",
        "            cleaned_results = process_image(image_path, model, structure_model, processor_trOCR, model_trOCR)\n",
        "\n",
        "            # Compare cleaned results with correct answers\n",
        "            score = 0\n",
        "            total = len(cleaned_results)\n",
        "            for i, token in enumerate(cleaned_results):\n",
        "                if token.lower() == correct_answers[i].lower():\n",
        "                    score += 1\n",
        "                elif token.lower() == \"uncertain\":\n",
        "                    if correct_answers[i].lower() in [\"true\", \"false\"]:\n",
        "                        score += 1\n",
        "\n",
        "            results.append([image_name, score])\n",
        "\n",
        "    # Write the results to a CSV file\n",
        "    with open('image_scores.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['image_name', 'pred_marks'])\n",
        "        for result in results:\n",
        "            writer.writerow(result)\n",
        "\n",
        "    print(\"Scores have been saved to image_scores.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "loZgH-k9BgJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}